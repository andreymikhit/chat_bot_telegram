# chat_bot_telegram
GeekBrains
---
### –°–æ–∑–¥–∞–¥–∏–º —á–∞—Ç-–±–æ—Ç—ã –Ω–∞ 4 –∏–Ω—Ç–µ–Ω—Ç–∞:
* –ü—Ä–æ—Å—Ç–∞—è –±–æ–ª—Ç–∞–ª–∫–∞
* –í–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞–º
* –ü–æ–∫—É–ø–∫–∞ —Ç–æ–≤–∞—Ä–∞
* –í–æ–ø—Ä–æ—Å–Ω–æ-–æ—Ç–≤–µ—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞ –º–µ–¥. —Ç–µ–º–∞—Ç–∏–∫—É (—Å–∏–º–ø—Ç–æ–º—ã-–¥–∏–∞–≥–Ω–æ–∑)
---
### Telegram-–±–æ—Ç
---

#### –î–∞—Ç–∞—Å–µ—Ç—ã

* –ú–µ–¥. https://huggingface.co/datasets/blinoff/medical_qa_ru_data
* –î–∞—Ç–∞—Å–µ—Ç –ø—Ä–∞–∑–¥–Ω–∏–∫–∏ https://github.com/ynkdir/holiday/blob/master/csv/russia%40calendar.live.com
* –ü–æ–∫—É–ø–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ https://gbcdn.mrgcdn.ru/uploads/asset/5209459/attachment/1b2f5aa57ff77e7c2d2ee26ceb09eb9e.csv
* –§–∞–π–ª Otvety.txt –ø–æ —Å—Å—ã–ª–∫–µ(1,7G) orig. https://drive.google.com/file/d/1DQL9ybca4US***/view

```py
# Update -U
!pip3 install gensim==3.6.0 --use-pep517
!pip3 install scikit-learn
# –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Ä—Å–∏–∏
!pip freeze
!python --version
!pip show python-telegram-bot
#.   RESTART KERNEL
#!pip install pyTelegramBotAPI --upgrade

```

### –ë–æ–ª—Ç–∞–ª–∫–∞
–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–¥–ª—è –±–æ–ª–µ–µ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç–∞ 20.7):
https://core.telegram.org/bots/api
https://core.telegram.org/bots/tutorial
https://docs.python-telegram-bot.org/en/v20.7/telegram.ext.updater.html
https://7bd.ru/python/python-reference/bad-python-telegram-bot/?ysclid=lq8lc05erg422565551
https://cloud.google.com/python/docs/reference/dialogflow/latest/upgrading
https://tproger.ru/articles/kak-uchit-python-s-nulya-s-udovolstviem-podklyuchaem-nejroset-google-dialogflow-k-vawemu-botu?ysclid=lq8ke3hk2g38339404
https://reintech.io/blog/how-to-create-a-chatbot-with-dialogflow

```py
#!pip install dialogflow
#!pip install google-cloud-dialogflow --upgrade  # Vers-20.7
# dialogflow_v2beta1   WORKS!!!
```

#### –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –±–æ—Ç–∞ –≤ Telegram:
* –ó–∞–ø—É—Å–∫–∞–µ–º Telegram –∏ –Ω–∞—Ö–æ–¥–∏–º BotFather (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é!).
* –ù–∞—á–∏–Ω–∞–µ–º —á–∞—Ç —Å BotFather –∏ –≤–≤–æ–¥–∏–º "/newbot".
* –°–ª–µ–¥—É–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º, —á—Ç–æ–±—ã –∑–∞–¥–∞—Ç—å –∏–º—è –∏ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –Ω–∞—à–µ–≥–æ –±–æ—Ç–∞.
* BotFather –≤—ã–¥–∞—Å—Ç —Ç–æ–∫–µ–Ω API. –ö–æ–ø–∏—Ä—É–µ–º —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω –∏ –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ –≤ –Ω–∞—à–µ–º –∫–æ–¥–µ

t.me/geekbrains***

Done! Congratulations on your new bot. You will find it at t.me/geekbrains_hw_bot. You can now add a description, about section and profile picture for your bot, see /help for a list of commands. By the way, when you've finished creating your cool bot, ping our Bot Support if you want a better username for it. Just make sure the bot is fully operational before you do this.

Use this token to access the HTTP API:

64680***:AAF4fN***

Keep your token secure and store it safely, it can be used by anyone to control your bot.
For a description of the Bot API, see this page: https://core.telegram.org/bots/api

```py
#-- coding: utf-8 -
MY_TG_TOKEN = '64680***:AAF4fN***'
```

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–æ–¥—É–ª—å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è telegram.ext, –Ω–∞—á–∏–Ω–∞—è —Å –≤–µ—Ä—Å–∏–∏ 20.x

https://docs-python.ru/packages/biblioteka-python-telegram-bot-python/  

```py
!pip3 install word2vec #gensim not works
!pip3 install FastText #gensim not works
!pip3 install requests
```

```py
import os
#from telegram import Update
from telegram.ext import Updater, CommandHandler, MessageHandler, CallbackContext, Filters #filters vers-20.7
import string
from pymorphy2 import MorphAnalyzer
from stop_words import get_stop_words
import annoy

#from gensim.models import Word2Vec, FastText, KeyedVectors #not works ==gensim 3.6.0

#from collections.abc import Mapping #not works
import pickle
import numpy as np
from tqdm.notebook import tqdm
import pandas as pd
import pickle

tqdm.pandas()

from pandarallel import pandarallel
pandarallel.initialize(progress_bar=True)

import logging
import json
```

```py
import gensim.downloader as api
api.info()['models'].keys()
word_vectors = api.load("glove-wiki-gigaword-100")  # –∑–∞–≥—Ä—É–∑–∏–º –ø—Ä–µ–¥—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤ –∏–∑ gensim-data

import mmap
import re

def get_num_lines(file_path):
    fp = open(file_path, "r+")
    buf = mmap.mmap(fp.fileno(), 0)
    lines = 0
    while buf.readline():
        lines += 1
    return lines

# orig. https://drive.google.com/file/d/1DQL9ybca4USImUD***/view

#https://drive.google.com/file/d/1p9f1Qtd2STyf***/view?usp=sharing
#!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1p9f1Qtd2STyfLUq***' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1p9f1Qtd2STyfLUqhqWmCEbd2-l747Mu2" -O Otvety.txt && rm -rf /tmp/cookies.txt
!wget -q https://drive.google.com/file/d/1p9f1Qtd2STy***/view?usp=sharing
```

```py
!ls -lh

N = get_num_lines("Otvety.txt")

get_num_lines("Otvety.txt")

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∞–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤ –≤ —Å—Ç—Ä–æ—á–Ω—ã–π –≤–∏–¥
if not os.path.isfile('prepared_answers.txt'):

    question = None
    written = False

    with open("prepared_answers.txt", "w") as fout:
        with open("Otvety.txt", "r") as fin:
            for line in tqdm(fin):
                if line.startswith("---"):
                    written = False
                    continue
                if not written and question is not None:
                    fout.write(question.replace("\t", " ").strip() + "===" + line.replace("\t", " "))
                    written = True
                    question = None
                    continue
                if not written:
                    question = line.strip()
                    continue
```


```py
# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
def preprocess_txt(line):
    spls = "".join(i for i in line.strip() if i not in exclude).split()
    spls = [morpher.parse(i.lower())[0].normal_form for i in spls]
    spls = [i for i in spls if i not in sw and i != ""]
    return spls
```



```py
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å—Ç–∞—Ç—å–∏ https://habr.com/ru/articles/738176/ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
def clean_text(input_text):

    # HTML-—Ç–µ–≥–∏: –ø–µ—Ä–≤—ã–π —à–∞–≥ - —É–¥–∞–ª–∏—Ç—å –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ HTML-—Ç–µ–≥–∏
    clean_text = re.sub('<[^<]+?>', '', input_text)

    # URL –∏ —Å—Å—ã–ª–∫–∏: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ URL –∏ —Å—Å—ã–ª–∫–∏
    clean_text = re.sub(r'http\S+', '', clean_text)

    #–≠–º–æ–¥–∂–∏ –∏ —ç–º–æ—Ç–∏–∫–æ–Ω—ã: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏ –≤ —Ç–µ–∫—Å—Ç
    #–í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    clean_text = emojis_words(clean_text)

    # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    clean_text = clean_text.lower()

    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã
    # –¢–∞–∫ –∫–∞–∫ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–æ–≤–∞–º–∏ - —É–¥–∞–ª–∏–º –ø—Ä–æ–±–µ–ª—ã
    clean_text = re.sub('\s+', ' ', clean_text)

    #–£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: –∏–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –≤—Å–µ–≥–æ, —á—Ç–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è "—Å–ª–æ–≤–∞–º–∏"
    clean_text = re.sub('[^–∞-—è–ê-–Ø—ë–Å0-9\s]', '', clean_text) #

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —á–∏—Å–ª–∞ –ø—Ä–æ–ø–∏—Å—å—é: 100 –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ "—Å—Ç–æ" (–¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–∞)# –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
    temp = inflect.engine()
    words = []
    for word in clean_text.split():
        if word.isdigit():
            words.append(num2words(int(word), lang='ru'))
        else:
            words.append(word)
    clean_text = ' '.join(words)

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞: —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ - —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤
    stop_words = set(stopwords.words('russian'))
    tokens = word_tokenize(clean_text)
    tokens = [token for token in tokens if token not in stop_words]
    clean_text = ' '.join(tokens)

    # –ó–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    clean_text = re.sub(r'[^\w\s]', '', clean_text)

    # –ò –Ω–∞–∫–æ–Ω–µ—Ü - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    return clean_text

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏ –≤ —Å–ª–æ–≤–∞
def emojis_words(text):

    # –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
    clean_text = emoji.demojize(text, delimiters=(" ", " "))

    # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º –∑–∞–º–µ–Ω—ã ":" –∏" _", –∞ —Ç–∞–∫ –∂–µ - –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    clean_text = clean_text.replace(":", "").replace("_", " ")

    return clean_text

def preproc_text(text):
    res = str(text).strip()
    res = re.sub(r"\s+", " ", res)
    return res

def build_data(data_q, data_ans):
    data = []
    for idx, texts in enumerate(data_q):
        question = preproc_text(texts)
        answer = preproc_text(data_ans.iloc[idx])
        res = '\nx:' + question + '\ny:' + answer
        data.append(res)
    return data
```

```py
# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞

sentences = []
morpher = MorphAnalyzer()
sw = set(get_stop_words("ru"))
exclude = set(string.punctuation)
exclude = {}
c = 0

file_path_from = 'prepared_answers.txt'
file_path_to = 'Otvety2.txt'

if not os.path.isfile(file_path_to):

    N = get_num_lines(file_path_from)
    with open(file_path_to, mode = 'w') as fileto:
        with open(file_path_from) as filefrom:
            for k in tqdm(range(N)):
                line = filefrom.readline()
                if line == '': break
                spls = preprocess_txt(line)
                sentences.append(spls)
                c += 1
                if c > 50_000:
                    break
                fileto.write(' '.join(spls)+'\n')
    filefrom.close()
    fileto.close()
```


```py
# –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç

sentences = []

file_path_from = 'Otvety2.txt'
if os.path.isfile(file_path_from):
    N = get_num_lines(file_path_from)
    with open(file_path_to, mode = 'r') as filefrom:
        for k in tqdm(range(N)):
            line = filefrom.readline()
            if line == '': break
            sentences.append(line.split())
    filefrom.close()
```


```py
vec = []
_ = [vec.extend(x)  for x in sentences[:100]]
vec = list(set(vec))
vec.sort()
vec[20:30]
```


```py
# –û—á–∏—Å—Ç–∏–º

sentences[:10]

#sentences = sentences[:50_000]

# –û–±—É—á–∏–º –º–æ–¥–µ–ª—å FastText

file_path_from = 'ft_model'
if not os.path.isfile(file_path_from):

    sentences = [i for i in tqdm(sentences) if len(i) > 2]
    modelFT = FastText(sentences=sentences, size=100, min_count=1, window=5)
    modelFT.save("ft_model")
    ft_index = annoy.AnnoyIndex(100 ,'angular')


# –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å

modelFT = FastText.load("ft_model")
ft_index = annoy.AnnoyIndex(100 ,'angular')


list(set(get_stop_words("ru")))[:20]

# –°–æ–∑–¥–∞–µ–º –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤-–æ—Ç–≤–µ—Ç–æ–≤

file_path_from = 'speaker.ann'
if not os.path.isfile(file_path_from):
    morpher = MorphAnalyzer()
    sw = set(get_stop_words("ru"))
    exclude = set(string.punctuation)
    modelFT = FastText.load("ft_model")
    ft_index = annoy.AnnoyIndex(100 ,'angular')

    index_map = {}
    counter = 0
    with open("Otvety2.txt", "r") as f:
        for line in tqdm(f):
            n_ft = 0
            spls = line.split("===")
            index_map[counter] = spls[1]
            question = preprocess_txt(spls[0])
            vector_ft = np.zeros(100)
            for word in question:
                if word in modelFT.wv:
                    vector_ft += modelFT.wv[word]
                    n_ft += 1
            if n_ft > 0:
                vector_ft = vector_ft / n_ft
            ft_index.add_item(counter, vector_ft)

            counter += 1

            if counter > 50_000:
                break

    ft_index.build(10)
    ft_index.save('speaker.ann')

    with open("index_speaker.pkl", "wb") as f:
        pickle.dump(index_map, f)

```



```py
spls
```

```py
#  –ó–∞–≥—Ä—É–∑–∏–º –∏–Ω–¥–µ–∫—Å—ã
ft_index = annoy.AnnoyIndex(100, 'angular')
ft_index.load('speaker.ann')
index_map = pd.read_pickle("index_speaker.pkl")
```


```py
np.random.permutation(100)

```


```py
#  –ü—Ä–∏–º–µ—Ä –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
a = ft_index.get_nns_by_vector(np.random.permutation(100), 5, include_distances=True)
a
```


```py
[index_map[x] for x in a[0]]
```

### –°–æ–∑–¥–∞–¥–∏–º –º–æ–¥–µ–ª—å –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

ProductsDataset.csv


```py
# https://gbcdn.mrgcdn.ru/uploads/asset/5209459/attachment/1b2f5aa57ff77e7c2d2ee26ceb09eb9e.csv
#https://drive.google.com/file/d/1b6324sAk2nkFGUdCl***/view?usp=sharing
!wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1b6324sAk2nk***' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1b6324sAk2nkFGUdCl1***" -O ProductsDataset.csv && rm -rf /tmp/cookies.txt

!ls -lh
```

```py
# –°–æ–∑–¥–∞–¥–∏–º –º–æ–¥–µ–ª—å –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

shop_data = pd.read_csv("ProductsDataset.csv")
#shop_data = shop_data.iloc[:5000, :]

shop_data['text'] = shop_data['title'] + " " + shop_data["descrirption"]
shop_data['text'] = shop_data['text'].progress_apply(lambda x: preprocess_txt(str(x)))
shop_data.head()
```

```py
# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–æ–º–µ–Ω–∞ –¥–∞–Ω–Ω—ã—Ö

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression

vectorizer = CountVectorizer(ngram_range=(1, 2))


idxs = set(np.random.randint(0, len(index_map), len(shop_data)))
# –í–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç–Ω—ã–π –¥–æ–º–µ–Ω
negative_texts = [" ".join(preprocess_txt(index_map[i])) for i in tqdm(idxs)]
# –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–π –¥–æ–º–µ–Ω
positive_texts = [" ".join(val) for val in tqdm(shop_data['text'].values)]



# –í–û = 0, –ü—Ä–æ–¥ = 1

dataset = negative_texts + positive_texts
labels = np.zeros(len(dataset))
labels[len(negative_texts):] = np.ones(len(positive_texts))



from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2,
                                                    stratify=labels, random_state=13)


# –ú–æ–¥–µ–ª—å

x_train_vec = vectorizer.fit_transform(X_train)
x_test_vec = vectorizer.transform(X_test)

lr = LogisticRegression().fit(x_train_vec, y_train)


# –ö–∞—á–µ—Å—Ç–≤–æ

from sklearn.metrics import accuracy_score
accuracy_score(y_true=y_test, y_pred=lr.predict(x_test_vec))


# –î–æ–±–∞–≤–∏–º IDF –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –Ω–∞–π–¥–µ–º IDF –≤–µ—Å)
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer().fit(X_train)

np.mean(tfidf_vect.idf_)

idfs = {v[0]: v[1] for v in zip(tfidf_vect.vocabulary_, tfidf_vect.idf_)}

len(idfs.keys())

list(idfs.keys())[:5]


# –°–æ–∑–¥–∞–µ–º –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

file_path_from = 'shop.ann'
if not os.path.isfile(file_path_from):


    ft_index_shop = annoy.AnnoyIndex(100 ,'angular')

    midf = np.mean(tfidf_vect.idf_)

    index_map_shop = {}
    counter = 0

    for i in tqdm(range(len(shop_data))):
        n_ft = 0
        index_map_shop[counter] = (shop_data.loc[i, "title"], shop_data.loc[i, "image_links"])
        vector_ft = np.zeros(100)
        for word in shop_data.loc[i, "text"]:
            if word in modelFT.wv:
                vector_ft += modelFT.wv[word] * idfs.get(word, midf)
                n_ft += idfs.get(word, midf)
        if n_ft > 0:
            vector_ft = vector_ft / n_ft
        ft_index_shop.add_item(counter, vector_ft)
        counter += 1

    ft_index_shop.build(10)
    ft_index_shop.save('shop.ann')

    file_path_from = 'index_shop.pkl'
    if not os.path.isfile(file_path_from):

        with open("index_shop.pkl", "wb") as f:
            pickle.dump(index_map_shop, f)



# –ó–∞–≥—Ä—É–∑–∏–º –∏–Ω–¥–µ–∫—Å—ã

midf = np.mean(tfidf_vect.idf_)

ft_index_shop = annoy.AnnoyIndex(100, 'angular')
ft_index_shop.load('shop.ann')

index_map_shop = pd.read_pickle("index_shop.pkl")


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä —Ö100

def embed_txt(txt, idfs, midf):
    n_ft = 0
    vector_ft = np.zeros(100)
    for word in txt:
        if word in modelFT.wv:
            vector_ft += modelFT.wv[word] * idfs.get(word, midf)
            n_ft += idfs.get(word, midf)
    return vector_ft / n_ft



# –ü—Ä–∏–º–µ—Ä –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞

ft_index_shop.get_nns_by_vector(np.ones(100)*20, 5, include_distances=True)



# Small preprocess of the answers

assert True

if not os.path.isfile('prepared_answers.txt'):

    question = None
    written = False

    with open("prepared_answers.txt", "w") as fout:
        with open("Otvety.txt", "r") as fin:
            for line in tqdm(fin):
                if line.startswith("---"):
                    written = False
                    continue
                if not written and question is not None:
                    fout.write(question.replace("\t", " ").strip() + "===" + line.replace("\t", " "))
                    written = True
                    question = None
                    continue
                if not written:
                    question = line.strip()
                    continue

```

```py
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å—Ç–∞—Ç—å–∏ https://habr.com/ru/articles/738176/ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
def clean_text(input_text):

    # HTML-—Ç–µ–≥–∏: –ø–µ—Ä–≤—ã–π —à–∞–≥ - —É–¥–∞–ª–∏—Ç—å –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ HTML-—Ç–µ–≥–∏
    clean_text = re.sub('<[^<]+?>', '', input_text)

    # URL –∏ —Å—Å—ã–ª–∫–∏: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ URL –∏ —Å—Å—ã–ª–∫–∏
    clean_text = re.sub(r'http\S+', '', clean_text)

    #–≠–º–æ–¥–∂–∏ –∏ —ç–º–æ—Ç–∏–∫–æ–Ω—ã: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏ –≤ —Ç–µ–∫—Å—Ç
    #–í–∞–∂–Ω–æ –ø–æ–Ω–∏–º–∞—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é –æ–∫—Ä–∞—Å–∫—É –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    clean_text = emojis_words(clean_text)

    # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    clean_text = clean_text.lower()

    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã
    # –¢–∞–∫ –∫–∞–∫ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ–ø–µ—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–ª–æ–≤–∞–º–∏ - —É–¥–∞–ª–∏–º –ø—Ä–æ–±–µ–ª—ã
    clean_text = re.sub('\s+', ' ', clean_text)

    #–£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: –∏–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –≤—Å–µ–≥–æ, —á—Ç–æ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è "—Å–ª–æ–≤–∞–º–∏"
    clean_text = re.sub('[^–∞-—è–ê-–Ø—ë–Å0-9\s]', '', clean_text) #

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —á–∏—Å–ª–∞ –ø—Ä–æ–ø–∏—Å—å—é: 100 –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ "—Å—Ç–æ" (–¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–∞)# –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
    temp = inflect.engine()
    words = []
    for word in clean_text.split():
        if word.isdigit():
            words.append(num2words(int(word), lang='ru'))
        else:
            words.append(word)
    clean_text = ' '.join(words)

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞: —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ - —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤
    stop_words = set(stopwords.words('russian'))
    tokens = word_tokenize(clean_text)
    tokens = [token for token in tokens if token not in stop_words]
    clean_text = ' '.join(tokens)

    # –ó–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è: –¥–∞–ª–µ–µ - —É–¥–∞–ª—è–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤—Å–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    clean_text = re.sub(r'[^\w\s]', '', clean_text)

    # –ò –Ω–∞–∫–æ–Ω–µ—Ü - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    return clean_text

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —ç–º–æ–¥–∂–∏ –≤ —Å–ª–æ–≤–∞
def emojis_words(text):

    # –ú–æ–¥—É–ª—å emoji: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∂–∏ –≤ –∏—Ö —Å–ª–æ–≤–µ—Å–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è
    clean_text = emoji.demojize(text, delimiters=(" ", " "))

    # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –ø—É—Ç—ë–º –∑–∞–º–µ–Ω—ã ":" –∏" _", –∞ —Ç–∞–∫ –∂–µ - –ø—É—Ç—ë–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–µ–ª–∞ –º–µ–∂–¥—É –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    clean_text = clean_text.replace(":", "").replace("_", " ")

    return clean_text

def preproc_text(text):
    res = str(text).strip()
    res = re.sub(r"\s+", " ", res)
    return res

def build_data(data_q, data_ans):
    data = []
    for idx, texts in enumerate(data_q):
        question = preproc_text(texts)
        answer = preproc_text(data_ans.iloc[idx])
        res = '\nx:' + question + '\ny:' + answer
        data.append(res)
    return data
```

### –¢–ï–õ–ï–ì–†–ê–ú-–ë–û–¢

```py
# –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–µ–≥–æ –±–æ—Ç–∞ –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–º
# @ botfather
# /start
# /newbot - create a new bot
# name1
# name1_BOT
#. ->. API


# –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Å–≤–æ–π —Ç–æ–∫–µ–Ω
updater = Updater(MY_TG_TOKEN, use_context=True) # –¢–æ–∫–µ–Ω API –∫ Telegram
dispatcher = updater.dispatcher

def startCommand(update, context):
    context.bot.send_message(chat_id=update.message.chat_id, text='–ü—Ä–∏–≤–µ—Ç! –ü–æ–±–æ–ª—Ç–∞–µ–º? üòä')

def textMessage(update, context):
    input_txt = preprocess_txt(update.message.text)
    vect = vectorizer.transform([" ".join(input_txt)])
    prediction = lr.predict(vect)

    # –ü–†–û–î
    if prediction[0] == 1:
        vect_ft = embed_txt(input_txt, idfs, midf)
        ft_index_shop_val = ft_index_shop.get_nns_by_vector(vect_ft, 5)
        for item in ft_index_shop_val:
            title, image = index_map_shop[item]
            context.bot.send_message(chat_id=update.message.chat_id, text="title: {} image: {}".format(title, image))
        return

    # QA
    vect_ft = embed_txt(input_txt, {}, 1)
    ft_index_val, distances = ft_index.get_nns_by_vector(vect_ft, 1, include_distances=True)

    #
    if distances[0] > 100.5:
        print(distances[0])
        context.bot.send_message(chat_id=update.message.chat_id, text="–ú–æ—è —Ç–≤–æ—è –Ω–µ –ø–æ–Ω–∏–º–∞—Ç—å")
        return

    # –í–æ–ø—Ä–æ—Å-–û—Ç–≤–µ—Ç
    context.bot.send_message(chat_id=update.message.chat_id, text=index_map[ft_index_val[0]])

#...
def weather(update, context):
    city = update.message.text
    res = requests.get(WEATHER_URL.format(city, WEATHER_API_KEY)) #WEATHER_API_KEY from openweathermap.org
    data = res.json()
    if data.get('cod') == 200:
        city_name = data['name']
        country = data['sys']['country']
        temperature = data['main']['temp']
        description = data['weather'][0]['description']

        weather_info = f"–ü–æ–≥–æ–¥–∞ –≤ {city_name}, {country}:\n–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temperature}¬∞C\n–û–ø–∏—Å–∞–Ω–∏–µ: {description}"
        update.message.reply_text(weather_info)
    else:
        update.message.reply_text('–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≤–≤–µ—Å—Ç–∏ –¥—Ä—É–≥–æ–π –≥–æ—Ä–æ–¥.')
# ...

start_command_handler = CommandHandler('start', startCommand)
text_message_handler = MessageHandler(Filters.text, textMessage)
dispatcher.add_handler(start_command_handler)
dispatcher.add_handler(text_message_handler)
updater.start_polling(clean=True)
updater.idle()
```

```py
# Dialogflow https://dialogflow.cloud.google.com
!pip install grpcio-status

#https://gbcdn.mrgcdn.ru/uploads/record/300116/attachment/f7bc1d4dd6610f0dd6caebdbb9424204.mp4

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã:
# https://github.com/Koziev/NLP_Datasets
```
